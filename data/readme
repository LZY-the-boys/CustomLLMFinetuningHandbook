


GPT4 dataset:

instruct format:

openhermes
- GPTeacher - General Instruct, Roleplay v1, Roleplay v2, and Code Instruct Datasets, by Teknium
- WizardLM (v1, evol_instruct 70k), by WizardLM Team/nlpxucan
- Airoboros GPT-4 (v1.0), by JonDurbin
- Camel-AI's domain expert datasets, by the Camel-AI Team
- CodeAlpaca, by Sahil2801
- GPT4-LLM and Unnatural Instructions, by Microsoft
---------------------------------------------------------------------------------------------------
- OpenOrca / OpenOrca2 / SlimOrca
- ShareGPT4
- OpenPlaytus-20k
- ehartford/dolphin
- ultrachat

chat format:
- WizardLM/WizardLM_evol_instruct_V2_196k
- [-] Open-Orca/SlimOrca-Dedup (36k) 
- [-] openchat/openchat_sharegpt4_dataset (3807) [clean!][worsest]
- [-] m-a-p/COIG-CQIA (44666)
- [-] teknium/GPTeacher-General-Instruct (83731)
- [-] teknium/GPT4-LLM-Cleaned (54568)
- [-] teknium/dataforge-economics (880)
- ~~sharegpt-zh~~
- HuggingFaceH4/ultrachat_200k
- jondurbin/airoboros-3.1
- Capybara 1 2 3
- GOAT
- Glaive
- MetaMathQA
- OpenAssistant
(https://huggingface.co/openchat/openchat_3.5)

Natural Dataset:

- trainset of gsm8k / MATH / bbq / MMLU / other qa (openbookqa,sciq,...)
- dolly / oasst 
- bbq
- [-] HuggingFaceH4/no_robots (10k)


DPO
- Intel/orca_dpo_pairs
- HuggingFaceH4/ultrafeedback_binarized
- [-] berkeley-nest/Nectar

Custom Dataset:

- hust dataset


v1216 (562677 items, sharegpt vicuna1.1)
v1221 (559599 items, sharegpt chatml)
-------
v1223 -> data mining version