
### Usage

1. install vllm first: 
```
git clone https://github.com/LZY-the-boys/vllm
pip install -e .
source activate vllm
```
2. start server
```
bash vllm.sh
# default start at  http://0.0.0.0:8000
```
3.  get the data

command line: 

```
# directly curl 
curl http://localhost:8000/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
"model": "lu-vae/qwen-openhermes-merged",
"messages": [
{"role": "system", "content": ""},
{"role": "user", "content": "Write a Perl script that processes a log file and counts the occurrences of different HTTP status codes. The script should accept the log file path as a command-line argument and print the results to the console in descending order of frequency."}
]
}'| echo -e "$(cat)\n" >> tmp.json
```

http getter:

```
#TODO: will implement a python client 
```

simple web ui:

```
# TODO: simple gradio version
```

release web ui:

```
# current web-ui version: https://github.com/enricoros/big-agi
```
- default system prompt: (disable chatgpt)
- bad conversation history?